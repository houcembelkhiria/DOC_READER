# ============================
# üß† NLP API - FastAPI Backend
# ============================

from fastapi import FastAPI
from pydantic import BaseModel
from transformers import pipeline, T5Tokenizer, T5ForConditionalGeneration
import spacy


# -------------------------------------------------------
# üöÄ Initialisation de l'application FastAPI
# -------------------------------------------------------
app = FastAPI(
    title="NLP API",
    description="API pour le r√©sum√© de texte et l'extraction d'entit√©s nomm√©es (NER)",
    version="1.0"
)


# -------------------------------------------------------
# üìò Chargement du mod√®le T5 (pour le r√©sum√©)
# -------------------------------------------------------
# Mod√®le T5 pr√©-entra√Æn√© de Hugging Face (petite version pour la rapidit√©)
MODEL_NAME = "t5-base" # or facebook/bart-large-cnn

# Chargement du tokenizer et du mod√®le
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)

# Cr√©ation d‚Äôun pipeline Hugging Face pour la t√¢che de r√©sum√©
#summarizer = pipeline("summarization", model=MODEL_NAME)
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")


# -------------------------------------------------------
# üß© Chargement du mod√®le spaCy pour la NER (anglais)
# -------------------------------------------------------
# Pour un mod√®le fran√ßais, ex√©cute au pr√©alable :
#   python -m spacy download fr_core_news_md
# puis remplace par :
#   nlp = spacy.load("fr_core_news_md")
nlp = spacy.load("en_core_web_sm")


# -------------------------------------------------------
# üì• Mod√®le de donn√©e d'entr√©e (Pydantic)
# -------------------------------------------------------
class TextInput(BaseModel):
    text: str


# -------------------------------------------------------
# ‚úÇÔ∏è Endpoint : R√©sum√© de texte
# -------------------------------------------------------
@app.post("/summarize")
def summarize(input_data: TextInput):
    """
    G√©n√®re un r√©sum√© du texte fourni √† l'aide du mod√®le T5.
    """
    text = input_data.text.strip()

    # Appel du pipeline Hugging Face pour le r√©sum√©
    summary = summarizer(
        text,
        max_length=100,   # Longueur maximale du r√©sum√©
        min_length=20,    # Longueur minimale du r√©sum√©
        do_sample=False   # Pas d‚Äô√©chantillonnage al√©atoire ‚Üí r√©sultat stable
    )[0]['summary_text']

    return {"summary": summary}


# -------------------------------------------------------
# üè∑Ô∏è Endpoint : Extraction d'entit√©s nomm√©es (NER)
# -------------------------------------------------------
@app.post("/ner")
def ner(input_data: TextInput):
    """
    Extrait les entit√©s nomm√©es (personnes, lieux, organisations, etc.)
    du texte fourni √† l‚Äôaide de spaCy.
    """
    text = input_data.text.strip()
    doc = nlp(text)

    # Extraction des entit√©s sous forme de dictionnaire
    entities = [{"text": ent.text, "label": ent.label_} for ent in doc.ents]

    return {"entities": entities}
